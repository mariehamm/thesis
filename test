

# Load your data
# data_combined: A DataFrame with columns 'species', 'latitude', 'longitude'
# hex_grid: A GeoDataFrame with polygons representing hexagons

# Create a GeoDataFrame for your species data
geometry = [Point(xy) for xy in zip(data_combined['longitude'], data_combined['latitude'])]
gdf_species = gpd.GeoDataFrame(data_combined, geometry=geometry)

# Ensure CRS (Coordinate Reference System) matches with the hex grid
gdf_species.set_crs(hex_grid.crs, allow_override=True)

# Perform spatial join: find which hexagon each species point falls in
gdf_joined = gpd.sjoin(gdf_species, hex_grid, how='left', op='within')

# Count unique species in each hexagon
species_richness = gdf_joined.groupby('hex_id')['species'].nunique().reset_index()

# species_richness will contain the species richness for each hexagon



# Load the Swiss boundary shapefile and transform to CRS 2056
swiss_boundary = gpd.read_file("/home/ubuntu/master/data/Swiss Boundary LV95/swissBOUNDARIES3D_1_5_TLM_LANDESGEBIET.shp")
swiss_boundary = swiss_boundary.to_crs(2056)

keep_columns = [
    'NAME', 'EINWOHNERZ', 'geometry'
]

# Select only the columns you want to keep
swiss_boundary = swiss_boundary[keep_columns]
swiss_boundary = swiss_boundary[swiss_boundary['NAME'] == 'Schweiz']

# Display first few rows
print(swiss_boundary.head())


## Hexagonal Grid accross CH
# Create the hexagonal grid

# Get bounding box coordinates
minx, miny, maxx, maxy = swiss_boundary.total_bounds

# Define hexagon size 
hex_size = 5000  
hex_width = 2* hex_size
hex_height = np.sqrt(3) * hex_size  #  height based on hex geometry

# Generate hexagonal grid
hexagons = []
x_start = minx
y_start = miny
row = 0

while y_start < maxy:
    x = x_start + (hex_width * 0.75* (row % 2))
    y = y_start

    while x < maxx:
        hexagon = Polygon([
            (x, y),
            (x + hex_width, y + hex_height / 2),
            (x + hex_width, y + hex_height * 1.5),
            (x, y + hex_height * 2),
            (x - hex_width, y + hex_height * 1.5),
            (x - hex_width, y + hex_height / 2),
            (x, y)  # Closing the polygon
        ])
        
        # Keep only hexagons that intersect with the boundary
        if swiss_boundary.geometry.unary_union.intersects(hexagon):
            hexagons.append(hexagon)

        x += hex_width * 1.5  # Move to the next hexagon in the row

    y_start += hex_height * 1.5  # Move to the next row
    row += 1

# Convert to GeoDataFrame
hex_grid_gdf = gpd.GeoDataFrame(geometry=hexagons, crs=swiss_boundary.crs)
hex_grid_gdf["hex_id"] = range(1, len(hex_grid_gdf) + 1)

# Save or display the result

hex_grid_gdf.to_file("/home/ubuntu/hex_grid.geojson", driver="GeoJSON")
#print(hex_grid_gdf)



# Create a list of unique species names per hexagon
species_names = gdf_joined.groupby('hex_id')['species'].apply(lambda x: ', '.join(sorted(set(x.dropna())))).reset_index()
species_names.rename(columns={'species': 'species_list'}, inplace=True)

# Create a list of unique class names per hexagon
class_names = gdf_joined.groupby('hex_id')['class'].apply(lambda x: ', '.join(sorted(set(x.dropna())))).reset_index()
class_names.rename(columns={'class': 'class_list'}, inplace=True)

# Merge all data into the hexagon grid
hex_grid_gdf = hex_grid_gdf.merge(species_richness, on="hex_id", how="left")
hex_grid_gdf = hex_grid_gdf.merge(species_names, on="hex_id", how="left")
hex_grid_gdf = hex_grid_gdf.merge(class_names, on="hex_id", how="left")


## shannon index
# Group by location and compute species richness and Shannon diversity
def compute_diversity(group):
    species_counts = group['species'].value_counts()
    shannon_index = entropy(species_counts)  # Shannon Diversity Index
    return pd.Series({'species_diversity': len(species_counts), 'shannon_index': shannon_index})

# Aggregate by location
diversity_df = combined_data.groupby(['decimalLatitude', 'decimalLongitude']).apply(compute_diversity).reset_index()

# Convert to a GeoDataFrame (Ensure CRS is set correctly)
geometry = [Point(lon, lat) for lon, lat in zip(diversity_df['decimalLongitude'], diversity_df['decimalLatitude'])]
gdf_diversity = gpd.GeoDataFrame(diversity_df, geometry=geometry)

gdf_diversity.set_crs(epsg=4326, allow_override=True, inplace=True)


# Convert to EPSG:2056 (Swiss coordinate system)
gdf_diversity = gdf_diversity.to_crs(epsg=2056)

# Save as GeoJSON for QGIS
gdf_diversity.to_file("/home/ubuntu/master/output/species_diversity.geojson", driver="GeoJSON")



wkt_geom	hex_id	species_points_count
Polygon ((2658000 1234086.2003928238991648, 2656500 1236684.27660417719744146, 2653500 1236684.27660417719744146, 2652000 1234086.2003928238991648, 2653500 1231488.12418147060088813, 2656500 1231488.12418147060088813, 2658000 1234086.2003928238991648))	961	8
Polygon ((2658000 1265263.11492906347848475, 2656500 1267861.19114041677676141, 2653500 1267861.19114041677676141, 2652000 1265263.11492906347848475, 2653500 1262665.03871771018020809, 2656500 1262665.03871771018020809, 2658000 1265263.11492906347848475))	967	7
Polygon ((2658000 1254870.8100836502853781, 2656500 1257468.88629500358365476, 2653500 1257468.88629500358365476, 2652000 1254870.8100836502853781, 2653500 1252272.73387229698710144, 2656500 1252272.73387229698710144, 2658000 1254870.8100836502853781))	965	6
Polygon ((2658000 1260066.96250635688193142, 2656500 1262665.03871771018020809, 2653500 1262665.03871771018020809, 2652000 1260066.96250635688193142, 2653500 1257468.88629500358365476, 2656500 1257468.88629500358365476, 2658000 1260066.96250635688193142))	966	6
Polygon ((2640000 1265263.11492906347848475, 2638500 1267861.19114041677676141, 2635500 1267861.19114041677676141, 2634000 1265263.11492906347848475, 2635500 1262665.03871771018020809, 2638500 1262665.03871771018020809, 2640000 1265263.11492906347848475))	833	5
Polygon ((2658000 1239282.35281553049571812, 2656500 1241880.42902688379399478, 2653500 1241880.42902688379399478, 2652000 1239282.35281553049571812, 2653500 1236684.27660417719744146, 2656500 1236684.27660417719744146, 2658000 1239282.35281553049571812))	962	4
Polygon ((2658000 1244478.50523823709227145, 2656500 1247076.58144959039054811, 2653500 1247076.58144959039054811, 2652000 1244478.50523823709227145, 2653500 1241880.42902688379399478, 2656500 1241880.42902688379399478, 2658000 1244478.50523823709227145))	963	4
Polygon ((2658000 1270459.26735177007503808, 2656500 1273057.34356312337331474, 2653500 1273057.34356312337331474, 2652000 1270459.26735177007503808, 2653500 1267861.19114041677676141, 2656500 1267861.19114041677676141, 2658000 1270459.26735177007503808))	968	4
Polygon ((2658000 1249674.65766094368882477, 2656500 1252272.73387229698710144, 2653500 1252272.73387229698710144, 2652000 1249674.65766094368882477, 2653500 1247076.58144959039054811, 2656500 1247076.58144959039054811, 2658000 1249674.65766094368882477))	964	3
Polygon ((2658000 1223693.89554741070605814, 2656500 1226291.97175876400433481, 2653500 1226291.97175876400433481, 2652000 1223693.89554741070605814, 2653500 1221095.81933605740778148, 2656500 1221095.81933605740778148, 2658000 1223693.89554741070605814))	959	2
Polygon ((2649000 1270459.26735177007503808, 2647500 1273057.34356312337331474, 2644500 1273057.34356312337331474, 2643000 1270459.26735177007503808, 2644500 1267861.19114041677676141, 2647500 1267861.19114041677676141, 2649000 1270459.26735177007503808))	903	1
Polygon ((2658000 1228890.04797011730261147, 2656500 1231488.12418147060088813, 2653500 1231488.12418147060088813, 2652000 1228890.04797011730261147, 2653500 1226291.97175876400433481, 2656500 1226291.97175876400433481, 2658000 1228890.04797011730261147))	960	1
Polygon ((2644500 1267861.19114041677676141, 2643000 1270459.26735177007503808, 2640000 1270459.26735177007503808, 2638500 1267861.19114041677676141, 2640000 1265263.11492906347848475, 2643000 1265263.11492906347848475, 2644500 1267861.19114041677676141))	868	0
Polygon ((2658000 1218497.74312470410950482, 2656500 1221095.81933605740778148, 2653500 1221095.81933605740778148, 2652000 1218497.74312470410950482, 2653500 1215899.66691335081122816, 2656500 1215899.66691335081122816, 2658000 1218497.74312470410950482))	958	0
Polygon ((2658000 1275655.4197744766715914, 2656500 1278253.49598582996986806, 2653500 1278253.49598582996986806, 2652000 1275655.4197744766715914, 2653500 1273057.34356312337331474, 2656500 1273057.34356312337331474, 2658000 1275655.4197744766715914))	969	0



## AHP

import pandas as pd
import numpy as np

def load_csv(file_path):
    """Load CSV file into a Pandas DataFrame"""
    df = pd.read_csv(file_path, index_col=0)
    return df

def geometric_mean(matrix):
    """Compute geometric mean for each row to get priority vector"""
    return np.prod(matrix, axis=1) ** (1 / len(matrix))

def normalize_matrix(matrix):
    """Normalize the pairwise comparison matrix"""
    col_sums = matrix.sum(axis=0)
    return matrix / col_sums

def compute_priority_weights(matrix):
    """Compute priority weights using the geometric mean method"""
    gm = geometric_mean(matrix)
    return gm / gm.sum()

def compute_consistency(matrix, weights):
    """Calculate Consistency Index (CI) and Consistency Ratio (CR)"""
    n = matrix.shape[0]
    lambda_max = np.sum((matrix @ weights) / weights) / n
    CI = (lambda_max - n) / (n - 1)
    
    # Random Consistency Index (RI) values for matrices of size 1-10
    RI_values = {1: 0.00, 2: 0.00, 3: 0.58, 4: 0.90, 5: 1.12, 6: 1.24,
                 7: 1.32, 8: 1.41, 9: 1.45, 10: 1.49}
    
    RI = RI_values.get(n, 1.49)  # Default to max RI if n > 10
    CR = CI / RI if RI > 0 else 0  # Avoid division by zero
    
    return CI, CR

def ahp_analysis(file_path):
    """Perform AHP analysis from a CSV file"""
    df = load_csv(file_path)
    matrix = df.to_numpy(dtype=float)  # Convert DataFrame to NumPy array

    weights = compute_priority_weights(matrix)
    CI, CR = compute_consistency(matrix, weights)

    print("\nAHP Results:")
    print("------------")
    for criterion, weight in zip(df.index, weights):
        print(f"{criterion}: {weight:.4f}")
    
    print("\nConsistency Check:")
    print(f"Consistency Index (CI): {CI:.4f}")
    print(f"Consistency Ratio (CR): {CR:.4f}")

    if CR <= 0.1:
        print("The matrix is consistent.")
    else:
        print("The matrix is NOT consistent. Consider reviewing responses.")

# Run the analysis
file_path = "ahp_survey_results.csv"  # Change to your actual CSV file path
ahp_analysis(file_path)



'''
Consistency Check seems wrong // new try
Correct one seems to be in ahptest,py
## Consistency Check
# 1. Calculate the maximum eigenvalue (lambda_max)
eigvals, eigvecs = np.linalg.eig(normalized_matrix)  # Compute eigenvalues and eigenvectors
lambda_max = np.max(eigvals)  # Take the largest eigenvalue

# Use the real part of lambda_max
lambda_max_real = np.real(lambda_max)

# 2. Calculate the Consistency Index (CI)
n = len(criteria_list)
CI = (lambda_max_real - n) / (n - 1)

# 3. Define the Random Consistency Index (RI) based on the matrix size
RI_dict = {
    1: 0, 2: 0, 3: 0.58, 4: 0.90, 5: 1.12, 6: 1.24, 7: 1.32, 8: 1.41, 9: 1.45, 10: 1.49
}

RI = RI_dict.get(n, 1.49)  # Default to 1.49 if n is larger than 10

# 4. Calculate the Consistency Ratio (CR)
CR = CI / RI

# Output the results
print(f"Maximum Eigenvalue (lambda_max): {lambda_max_real:.4f}")
print(f"Consistency Index (CI): {CI:.4f}")
print(f"Random Consistency Index (RI) for n={n}: {RI}")
print(f"Consistency Ratio (CR): {CR:.4f}")

# Check if the consistency ratio is acceptable
if CR <= 0.1:
    print("The consistency ratio is acceptable (CR <= 0.1).")
else:
    print("The consistency ratio is too high (CR > 0.1), indicating inconsistency in the comparisons.")

det = np.linalg.det(ahp_matrix)
print(f"Determinant of AHP Matrix: {det}")


Maximum Eigenvalue (lambda_max): 1.0000
Consistency Index (CI): -1.0000
Random Consistency Index (RI) for n=8: 1.41
Consistency Ratio (CR): -0.7092
The consistency ratio is acceptable (CR <= 0.1).
Determinant of AHP Matrix: -5.286584362139931

High Consistency Ratio (CR): The CR value being high (11436.8882) is a clear indication that the matrix 
might not be consistent. Normally, if the CR > 0.1, you should consider revising the pairwise comparisons.

Inconsistent Judgments: The pairwise comparisons might have conflicting or inconsistent ratings. 
Error in Data: Sometimes, incorrect or poorly made pairwise comparisons may cause inconsistency. 
You should inspect the raw pairwise comparison data (CSV) to check for errors or mismatched judgments.
'''


## Calculate priority weights (average row method)
# Sum each column
column_sums = np.sum(ahp_matrix, axis=0)

# Normalize by dividing each element by the column sum
normalized_matrix = ahp_matrix / column_sums

# Compute the average of each row
priority_weights = np.mean(normalized_matrix, axis=1)

# Print results
print("Normalized AHP Matrix:")
print(normalized_matrix)

print("\nPriority Weights:")
print(priority_weights)

'''
20250310
Normalized AHP Matrix:
[[0.10526316 0.18181818 0.10714286 0.12987013 0.05       0.08823529
  0.08333333 0.08823529 0.10526316 0.10526316]
 [0.05263158 0.09090909 0.10714286 0.12987013 0.05       0.08823529
  0.08333333 0.08823529 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.10714286 0.06493506 0.15       0.17647059
  0.08333333 0.17647059 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.21428571 0.12987013 0.25       0.17647059
  0.08333333 0.17647059 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.03571429 0.02597403 0.05       0.02941176
  0.08333333 0.02941176 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.05357143 0.06493506 0.15       0.08823529
  0.08333333 0.08823529 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.10714286 0.12987013 0.05       0.08823529
  0.08333333 0.08823529 0.05263158 0.05263158]
 [0.10526316 0.09090909 0.05357143 0.06493506 0.15       0.08823529
  0.08333333 0.08823529 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.10714286 0.12987013 0.05       0.08823529
  0.16666667 0.08823529 0.10526316 0.10526316]
 [0.10526316 0.09090909 0.10714286 0.12987013 0.05       0.08823529
  0.16666667 0.08823529 0.10526316 0.10526316]]

Priority Weights:
[0.10444246 0.09008839 0.1165051  0.14371289 0.06605437 0.0935009
 0.08482523 0.0935009  0.10368488 0.10368488]

'''


'''
20250310
Criteria with Priority Weights:
                            Criteria  Priority Weight
0            biotic composition (bc)         0.104442
1            ecosystem function (ef)         0.090088
2        habitat connectivity (ef_2)         0.116505
3             habitat quality (ef_1)         0.143713
4  human recreation potential (ef_5)         0.066054
5        pollinator abundance (ef_3)         0.093501
6            red list species (bc_3)         0.084825
7                soil quality (ef_4)         0.093501
8           species diversity (bc_2)         0.103685
9            species richness (bc_1)         0.103685

'''

# n is the number of criteria (8)
n = len(ahp_matrix)

# Calculate the eigenvalues of the matrix
eigvals, _ = np.linalg.eig(ahp_matrix)

#Find the dominant eigenvalue (Î»_max)
lambda_max = np.real(np.max(eigvals))

#Calculate the Consistency Index (CI)
CI = (lambda_max - n) / (n - 1)

# Calculate the Random Consistency Index (RI) for n=8
RI = 1.41  # This is the standard value for n=8 (Saaty 1980)

# Calculate the Consistency Ratio (CR)
CR = CI / RI

# Output the results
print(f"Maximum Eigenvalue (lambda_max): {lambda_max}")
print(f"Consistency Index (CI): {CI}")
print(f"Random Consistency Index (RI) for n=8: {RI}")
print(f"Consistency Ratio (CR): {CR}")

# Step 7: Check if the consistency ratio is acceptable
if CR <= 0.1:
    print("The consistency ratio is acceptable (CR <= 0.1).")
else:
    print("The consistency ratio is not acceptable (CR > 0.1).")


'''
20250310
Maximum Eigenvalue (lambda_max): 10.579298505762573
Consistency Index (CI): 0.06436650064028587
Random Consistency Index (RI) for n=8: 1.41
Consistency Ratio (CR): 0.04565000045410346
The consistency ratio is acceptable (CR <= 0.1).
'''


'''
# Compute total weights for BC & EF
bc_total = sum(criteria_weights[c] for c in bc_criteria)
ef_total = sum(criteria_weights[c] for c in ef_criteria)

# Normalize BC & EF weights so they sum to 1
total_weight = bc_total + ef_total
bc_weight_norm = bc_total / total_weight
ef_weight_norm = ef_total / total_weight

# Normalize sub-criteria weights (so each is relative to BC or EF)
bc_sub_weights = {c: criteria_weights[c] / bc_total * bc_weight_norm for c in bc_criteria}
ef_sub_weights = {c: criteria_weights[c] / ef_total * ef_weight_norm for c in ef_criteria}

final_weights = {**bc_sub_weights, **ef_sub_weights}

# Print final normalized weights
print("\n Final Normalized Weights for Raster Calculation:")
for criterion, weight in final_weights.items():
    print(f"{criterion}: {weight:.6f}")
'''
'''
20250310
Final Normalized Weights for Raster Calculation:
species richness (bc_1): 0.128726
species diversity (bc_2): 0.128726
red list species (bc_3): 0.105311
habitat quality (ef_1): 0.178422
habitat connectivity (ef_2): 0.144642
pollinator abundance (ef_3): 0.116083
soil quality (ef_4): 0.116083
human recreation potential (ef_5): 0.082007
'''


for ahp 20250310

High-Level AHP Matrix:
[[1.  2. ]
 [0.5 1. ]]

High-Level Priority Weights:
                  Criteria    Weight
0  biotic composition (bc)  0.666667
1  ecosystem function (ef)  0.333333
ahp.py:72: RuntimeWarning: invalid value encountered in scalar divide
  CR = CI / RI

High-Level Consistency: lambda_max=2.0000, CI=0.0000, RI=0, CR=nan
Consistency not acceptable

BC Sub-Criteria AHP Matrix:
[[1.  1.  2. ]
 [1.  1.  2. ]
 [0.5 0.5 1. ]]

BC Sub-Criteria Local Weights:
                   Criteria  Local Weight
0   species richness (bc_1)           0.4
1  species diversity (bc_2)           0.4
2   red list species (bc_3)           0.2

BC Consistency: lambda_max=3.0000, CI=0.0000, RI=0.58, CR=0.0000
Consistency acceptable

EF Sub-Criteria AHP Matrix:
[[1.         2.         2.         2.         5.        ]
 [0.5        1.         2.         2.         3.        ]
 [0.5        0.5        1.         1.         3.        ]
 [0.5        0.5        1.         1.         3.        ]
 [0.2        0.33333333 0.33333333 0.33333333 1.        ]]

EF Sub-Criteria Local Weights:
                            Criteria  Local Weight
0             habitat quality (ef_1)      0.360732
1        habitat connectivity (ef_2)      0.250512
2        pollinator abundance (ef_3)      0.162074
3                soil quality (ef_4)      0.162074
4  human recreation potential (ef_5)      0.064607

EF Consistency: lambda_max=5.0816, CI=0.0204, RI=1.12, CR=0.0182
Consistency acceptable

Final Global Weights:
                            Criteria  Global Weight
0            species richness (bc_1)       0.266667
1           species diversity (bc_2)       0.266667
2            red list species (bc_3)       0.133333
3             habitat quality (ef_1)       0.120244
4        habitat connectivity (ef_2)       0.083504
5        pollinator abundance (ef_3)       0.054025
6                soil quality (ef_4)       0.054025
7  human recreation potential (ef_5)       0.021536

Total Global Weight Sum: 1.000000






'''
# Extract unique criteria from the questions, ignoring "bc" and "ef"
criteria_set = set()
for question in pairwise_questions:
    try:
        # Normalize criteria names
        criteria_pair = [c.strip().replace("?", "").replace("\n", "").lower() for c in question.split(":")[1].split(" or ")]
        criteria_set.update(criteria_pair)

        # Ensure "bc" and "ef" are captured if they are now compared
        if "biotic composition" in criteria_pair or "ecosystem function" in criteria_pair:
            criteria_set.update(["biotic composition (bc)", "ecosystem function (ef)"])
    
    except IndexError:
        print(f"Skipping malformed question: {question}")
    

## Ensure "biotic composition (bc)" and "ecosystem function (ef)" appear first
criteria_list = sorted(criteria_set, key=lambda x: (x not in ["biotic composition (bc)", "ecosystem function (ef)"], x))
criteria_index = {c.lower(): i for i, c in enumerate(criteria_list)}

print("Filtered Criteria (Used in AHP Matrix):", criteria_list)

# Verify criteria list before building the AHP matrix
n = len(criteria_list)  # Update n to reflect the correct number of criteria

# Initialize the AHP matrix
# Verify criteria list before building the AHP matrix
n = len(criteria_list)  # Update n to reflect the correct number of criteria

# Initialize the AHP matrix
ahp_matrix = np.ones((n, n))  # Identity matrix since self-comparison = 1

# Fill the matrix with pairwise comparisons
for _, row in df.iterrows():
    for question_col, importance_col in zip(pairwise_questions, importance_values):
        try:
            criteria_pair = [c.strip().replace("?", "").replace("\n", "").lower() for c in question_col.split(":")[1].split(" or ")]
            if len(criteria_pair) == 2:
                c1, c2 = criteria_pair
                
                # Handle cases where high-level criteria (bc, ef) are compared
                if "biotic composition" in c1 or "ecosystem function" in c1:
                    c1 = "biotic composition (bc)" if "biotic composition" in c1 else "ecosystem function (ef)"
                if "biotic composition" in c2 or "ecosystem function" in c2:
                    c2 = "biotic composition (bc)" if "biotic composition" in c2 else "ecosystem function (ef)"

                # Get importance rating
                importance = row[importance_col]
                if pd.notna(importance):  # Ignore empty responses
                    importance = float(importance)
                    
                    if c1 in criteria_index and c2 in criteria_index:
                        i, j = criteria_index[c1.lower()], criteria_index[c2.lower()]
                        ahp_matrix[i, j] = importance
                        ahp_matrix[j, i] = 1 / importance if importance != 0 else 1

                    else:
                        print(f"Skipping unknown criteria: {c1}, {c2}")
        except Exception as e:
            print(f"Error processing row: {e}")

# Print the final matrix for BC vs EF
print("AHP Matrix:")
print(ahp_matrix)

# Now, calculate priority weights for both BC/EF and the lower-level criteria
eigenvalues, eigenvectors = np.linalg.eig(ahp_matrix)
max_eigenvalue_index = np.argmax(eigenvalues)
principal_eigenvector = np.abs(eigenvectors[:, max_eigenvalue_index])
priority_weights = principal_eigenvector / np.sum(principal_eigenvector)



# Compute priority weights using the Eigenvector Method
eigenvalues, eigenvectors = np.linalg.eig(ahp_matrix)
max_eigenvalue_index = np.argmax(eigenvalues)
principal_eigenvector = np.abs(eigenvectors[:, max_eigenvalue_index])
priority_weights = principal_eigenvector / np.sum(principal_eigenvector)



# Display priority weights
criteria_df = pd.DataFrame({
    'Criteria': criteria_list,
    'Priority Weight': priority_weights
})
print("\nCriteria with Priority Weights (Eigenvector Method):")
print(criteria_df)

'''


'''


## Consistency Check (old version in test)
# Mu & Pereyra-Rojas (2017) 

lambda_max = np.real(np.max(eigenvalues))
CI = (lambda_max - n) / (n - 1)
RI = 1.41  # Standard RI for n=8
CR = CI / RI

print(f"Maximum Eigenvalue (lambda_max): {lambda_max}")
print(f"Consistency Index (CI): {CI}")
print(f"Random Consistency Index (RI) for n=8: {RI}")
print(f"Consistency Ratio (CR): {CR}")

if CR <= 0.1:
    print("The consistency ratio is acceptable (CR <= 0.1).")
else:
    print("The consistency ratio is not acceptable (CR > 0.1).")


Number of criteria (n): 8
Maximum Eigenvalue (lambda_max): 8.52899563017638
Consistency Index (CI): 0.0755708043109115
Random Consistency Index (RI) for n=8: 1.41
Consistency Ratio (CR): 0.053596315114121634
The consistency ratio is acceptable (CR <= 0.1).




# Normalize BC & EF weights
criteria_weights = dict(zip(criteria_list, priority_weights))
bc_criteria = ["species richness (bc_1)", "species diversity (bc_2)", "red list species (bc_3)"]
ef_criteria = ["habitat quality (ef_1)", "habitat connectivity (ef_2)", "pollinator abundance (ef_3)", 
               "soil quality (ef_4)", "human recreation potential (ef_5)"]

bc_total = sum(criteria_weights[c] for c in bc_criteria)
ef_total = sum(criteria_weights[c] for c in ef_criteria)
total_weight = bc_total + ef_total
bc_weight_norm = bc_total / total_weight
ef_weight_norm = ef_total / total_weight

bc_sub_weights = {c: criteria_weights[c] / bc_total * bc_weight_norm for c in bc_criteria}
ef_sub_weights = {c: criteria_weights[c] / ef_total * ef_weight_norm for c in ef_criteria}
final_weights = {**bc_sub_weights, **ef_sub_weights}

print("\nFinal Normalized Weights for Raster Calculation:")
for criterion, weight in final_weights.items():
    print(f"{criterion}: {weight:.6f}")

Final Normalized Weights for Raster Calculation:
species richness (bc_1): 0.129218
species diversity (bc_2): 0.129218
red list species (bc_3): 0.102097
habitat quality (ef_1): 0.193987
habitat connectivity (ef_2): 0.148826
pollinator abundance (ef_3): 0.113489
soil quality (ef_4): 0.113489
human recreation potential (ef_5): 0.069677
'''